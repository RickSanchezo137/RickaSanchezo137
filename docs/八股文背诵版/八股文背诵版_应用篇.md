# [返回](/)

# 八股文背诵版之—应用篇

## 故障分析&线上排查

### :point_right:**cpu使用率飙升排查？**

window利用任务管理器查看，主要是Linux：

1. 终端运行top指令，shift+p按cpu占用率从上到下排序，找到对应的pid
2. 使用top -H -p pid进入线程模式，会显示对应进程下的所有线程，终端对应的pid字段为线程号
3. 线程号转换成16进制，记为nid
4. 使用jstack pid > /tmp/info.dat输出所有堆栈信息
5. 打开该文件info.dat，查找nid对应部分的信息

### :point_right:OOM排查？

我的思路是，首先可以根据jps指令找到对应Java程序的进程pid，接下来有几种方案：① jmap -histo:live pid，实时打印出堆内存的各个对象占用的内存大小，最快地定位OOM的源头，不过这样可能会因Full GC而导致卡顿，对于线上的应用，可能会造成一些不可预计的后果；② 或者使用jmap -dump:format=b,file=xxx pid将实时堆内存情况dump下来，查看dump文件进行分析，可以结合一些MAT、VisualVM类似的可视化工具，但dump的时候也会导致卡顿；③ 或者结合jstack pid打印出堆栈信息，结合程序代码排查。对于不必要的大对象或很多的没用的小对象，及时回收，比如令引用指向null，threadLocal.remove等等，防止内存泄漏发生；对象或程序没什么问题，可以尝试增加堆内存初始化的值，-Xms和-Xmx一起设置为同一个大一点的值

### :point_right:HTTP超时排查？

客户端出现HTTP超时，我觉得可以从这几个方面排查，首先有可能是由于服务端数据发送过快或过频繁，客户端防火墙性能跟不上，或者有可能是由于服务端性能过载，导致请求被阻塞，后面请求不断丢包导致的。可以通过这样的思路去排查：比如说关闭客户端防火墙，查看服务端是否有过载情况发生，比如说用top指令查看内存利用率、cpu占有率等等，或者我印象里chrome有个火焰图的工具，可以在客户端用这个详细记录请求耗时，包括是哪个请求、哪个函数上发生了超时或者异常等等，准确定位到问题出现的源头

## 分布式场景

### :point_right:高并发高负载应用通用处理？

高并发高负载的应用，首先需要结合系统的基本特征来考虑嘛，这种系统首先要考虑并发性、其次是可用性、然后是容错性、最后是响应的时间。对于并发性的保证，我们可以利用分布式节点具有伸缩性的特征，对于高并发量的服务进行节点的水平扩展，再结合负载均衡的机制将流量分摊到每一个节点，提高整个系统能够承载的并发量；同时对于数据库节点，也可以做一些从节点，实现读写分离，或者对于大表查询大库查询，做好分库分表，设置一些缓存等等，降低数据库负担并提高并发能力；或者在高峰期对流量做一些削峰，比如说加入消息队列异步削峰等等；同时也要保证并发安全性，比如对于超卖这种场景，可以采用分布式锁这种处理方式保证并发安全性。对于可用性来说，我们就要求业务节点在出现问题的情况下，也要保证系统的及时响应，比如发现某个服务节点挂了，及时做出服务的熔断或者降级；对于大库大表的查询进行分库分表、读写分离、索引优化等，避免请求太久，然后阻塞导致响应时间太长。容错则要求有自动容错恢复的能力，对于数据库节点这种，合理进行数据库备份、主从同步等等；对于缓存节点，搭建好集群，设置哨兵模式对主节点进行监控，保证其容错恢复能力，或者在节点挂了的时候进行一个请求的重定向，就像Redis的集群重定向那样*（可能可以靠代理中间件来实现？）*。而对于响应的时间，可以根据用户IP，分配距离最近的服务节点等等

### :point_right:分库分表怎么操作？怎么迁移数据？跨库关联查询的问题？

首先对于垂直分表来说，我们需要对大表进行拆分，将热点字段拆分到一张表中，冷字段放到另一张表中，避免对冷数据的查找拖慢性能；对于垂直分库来说，我们需要做到专库专用，将一些不同的业务表放在不同的库当中，降低大库的数据访问压力；随着业务量逐渐增加，单个业务表也慢慢变大，随之要做一些水平拆分，水平分表之后，可以制定相应的规则，给表编个序号，然后修改相应客户端的语句，比如id在多少到多少之间的分配到某个表上，id在另一个范围分配到另一个表上，不过这样需要修改客户端逻辑，可以采取添加一个中间层代理客户端，来存储这种映射关系，客户端统一去访问这个代理客户端

迁移数据的话，可以停机迁移；开机迁移的话，我在考虑是否能借鉴Redis集群重定向这样一种规则，比如查到不对就返回一个ASK标识并且携带正确目标的地址，然后客户端重定向，或者代理客户端重定向去正确位置查询

跨库关联查询可以考虑做全局表，

### :point_right:微服务中一个服务节点挂了，怎么保证最终一致性？



## 个人项目

### :point_right:**简单介绍一下项目？在项目中的工作？**

1. 首先是网络理政中心项目，项目的来源是成都市高新区网络理政中心。主要的任务场景是这样的，高新区理政中心现场有三个由若干个小屏构成的大屏，每个小屏后面装有温度传感器，我们要做的是根据传感器收集到的数据来判断大屏有没有出现温度过高导致安全隐患的情况，并通过可视化技术展示出来。大概流程是这样的：传感器采集数据之后发送给mqtt broker，mqtt broker的订阅端接收数据并发送到mongodb数据库，我们的可视化平台从数据库中查询数据，并发送到前端展示

   我在项目中的主要职责是可视化平台后端的开发，其中我主要负责出入参日志的打印及用户操作记录、传感器历史数据的查询、大屏实时温度的查询

   - 出入参日志的打印及用户操作记录，主要采用了AOP和ThreadLocal。AOP采用了自定义注解@Weblog，并定义了一个切面类叫做WeblogAspect类，加上@Aspect注解，设置切点为Weblog注解，切面类中定义了一个ThreadLocal\<StringBuilder>对象。最后，将在每个方法前面添加该注解，每个请求线程相互隔离的往StringBuilder对象中添加日志，最后统一打印出来，实现日志打印的隔离有序；用户操作记录结合了过滤器和单例的ThreadLocal，在过滤器的doFilterInternal中首先验证token*（放入redis实现过期）*，接着根据token中解析出email信息在数据库中查询到user信息，将user对象用全局的ThreadLocal封装，接下来进入切面类定义的方法，根据JointPoint对象、Request对象以及User对象将方法名、IP地址、用户信息等等都存放到数据库中，实现用户操作的记录。不过想到可能会面对线程复用的问题*（Tomcat线程池）*

   - 传感器的历史查询主要采用redis作为缓存、mongodb作为持久化数据库来实现的。整体流程是这样的：前端发送查询历史数据的请求，缓存和mongodb以责任链的模式来处理请求，首先构造一个责任链，第一个handler为BufferHistoryHandler，从缓存中取，如果取的不够或没取到，则调用下一个handler DBHistoryHandler从数据库中取，并将结果返回给这个handler，推送到redis中去，同时删除redis中过老的数据，除此之外还会定时删除老数据，相当于是定时策略和惰性策略的结合，再返回给前端

     我是使用redis进行设备历史记录的缓存，最初，我使用简单的hash储存历史记录，一个key对应一个时间点，由于历史记录是一个时间线性结构，因此hash并不能很好的处理时间顺序。后来我改用zset结构，使用时间戳作为score，解决了上述问题

     此外, 由于设备状态采样周期约为10s，对于长时间的历史记录查询，之前由于返回给前端过长的数据列表，造成显示卡顿，我还实现了多级缓存的机制，即对于每个设备使用3个zset，分别储存1小时，1天和1个月的历史记录，每个zset对应于不同采样率，例如1个月的历史记录查询，则将原始数据重采样为一小时一次，并储存在redis中，减轻网络负担

   - 大屏实时温度的查询，前端传入大屏设备ID参数，首先想的思路是直接从数据库中拿，也就是以设备ID、存入时间戳的降序、温度，构建联合索引，取第一条返回，加快查询速度。查询采用线程池开启多个线程同时查，每个线程负责不同的设备ID范围，先用的是一个ArrayList，结果前端显示有几块屏数据丢失，意识到出现并发安全问题，采用CopyOnWriteArrayList，意识到操作会往容器进行很多次写操作，进行了很多次底层数组的拷贝，可能会触发频繁gc，而且synchronized方法影响了并发性能，最后改成每个线程往自己对应的ArrayList中存，最后再合并返回

2. 然后是四川大学插排项目，主要是从华为智能插排获取用电数据监测安全情况，并通过华为云平台下发指令控制插排

   我的主要工作是后端的开发，负责的功能点有历史数据的查询、Redis限流、SSE推送、出入参日志的打印及用户操作记录。历史数据的查询代码和上面有一定程度的复用，采取策略模式和不同的第三方平台对接并取历史数据，有华为、曼顿空开以及tuya等等

   - 项目管理的各种设备分散于不同的物联网平台中，如华为物联网平台，tuya物联网平台，曼顿物联网平台，每种平台都有其独特的对接方法。学习到了策略模式后，我就将不同平台的相同操作抽象出来变为接口，并用对应平台的逻辑加以实现，在调用时，使用一个策略选择器调用对应平台的策略，屏蔽了物联网平台的差异性，简化了代码
   - 关于Redis限流见难点

### :point_right:**技术选型怎么做的？为什么不用微服务架构？**

考虑到SpringBoot是一种业界广泛使用多年的、避免配置地狱的高效率的Java Web开发框架，我们采用了SpringBoot作为项目开发的主要框架；由于mongodb是非关系型数据库，因为数据库的字段是我们自己设计的，所以mongodb在字段的伸缩性上面表现的更强一些，同时对复杂数据类型支持的更好，比如说对象类型；由于我们要查询大量历史数据，因此需要加入Redis缓存提高查询效率，同时实现一些额外的功能；由于docker部署的简便性，我们采用docker容器的方式来实现项目各组件的运行。为什么不采用微服务的架构？是因为我们面临的场景，首先，不涉及到高并发，因为登陆的人员可能只有寥寥一两个，同时如果采用微服务的架构，各服务模块之间相互通过HTTP协议进行通信，速度可能还比不上在单体应用当中，况且我们的数据量也并不是很大，主要是历史数据的数量，我们目前的需求是需要一个月之内的历史数据，单一mongodb的节点完全可以实现这种级别的查询需求，因此，采用了单体的架构

### :point_right:**数据库怎么设计的？索引怎么设计的？数据量大不大？怎么考虑的？**



### :point_right:**项目难点有哪些？**可能存在哪些问题？有什么想法？

说说出现过的几个bug吧

1. Redis限流

   当时在插排项目的时候，因为我们要去调用华为的平台去查询历史数据嘛，有一天发现报错，华为平台返回的消息是“连接被阻断，可能是请求过快？”。然后我就去华为的官方文档去查，说是查询历史记录的接口每60秒只能调用100次。然后我就开始思考解决策略，首先我会尝试自己的想法，也就是用时间戳作为键，任意值作为value，过期时间设置为60s，然后调用keys *命令去看数量，等于100就不再去取了，这个想法乍一看没问题，但首先，它的键要不断删除，消耗redis cpu，其次，keys *命令是一个O(N)的，如果数据量大可能会阻塞整个Redis的事件循环

   所以接着去Redis官方文档去找，官方文档提供了两个Rate Limiter，第一个就是ip+timestamp作为键，这个时间里调用一次，value就加一，大于阈值就限流，同时设置一个过期时间，自动清除。迁移到我的应用里，因为我是自己限自己的流嘛，所以就不加ip，可以让timestamp/60000作为键，100作为阈值，设置个过期时间，但这样想想也不可以，在上个60秒的最后一秒内假如发了100个请求，这个键到阈值了，紧接着会创建下一个60秒的键，在第一秒发了100个请求，所以两秒之内发了200次请求，肯定是不可以的

   于是就去看第二个Rate Limiter，相当于是将ip作为键，一秒内不断增加value，达到阈值就限流，一秒后键过期，这种其实也不能满足，比如最后半秒达到阈值，接着后半秒又达到新键的阈值，又是一个时间段内达到了两倍的请求量，所以不行

   于是就去学习限流方法，先看到的是滑动窗口法，就是将时间段分成若干个小窗口，比如我令timestamp/10000作为键，过期时间设为10秒，将6个小窗口的value累加，加起来超过100，就限流，其实这样也会面临边界问题，比如键快过期的一刹那达到请求阈值，接着键过期，同样会造成无法限流的后果，只不过窗口粒度越细，概率越小，所以还是不够完美

   接着是漏桶算法，漏桶以一定的速度释放请求，如果请求过快，水桶就会满，拒绝请求。但这个方法释放请求的速度是恒定的，无法应对突发请求

   接着是令牌桶算法，桶是空的，不断往其中添加令牌，超过桶容量的就丢弃，每次来一个请求，就消费一个令牌，这样也可以应对突发流量，这个方法就比较完善了，比如可以设置成，桶的容量为40，极端情况为，一瞬间消费40个令牌，后面的60秒，设置为平均每秒只能消费一个令牌，就可以限量又限速

   最后我采用了zset的方法，将时间戳同时作为value和score，过期时间设置为60秒。每次计算zset的大小时，直接调用zcard，这是O(1)的，同时还可以调用zrembyRange将过期键删除

2. 查询最新数据

   

3. 线上查错

   

### :point_right:**有没有线上排查经历？**



### :point_right:**讲讲mongodb？**



### :point_right:**讲讲AOP？**



### :point_right:**讲讲跳表？**



### :point_right:**讲讲策略模式和责任链模式？**



### :point_right:**SSE？**

前端EventSource对象，content-type:text/event-stream，后端sseemitter，return之后连接就会断开，可以使用while循环维持一个长连接，写到响应体后通过flush写。sse有retry参数，return会导致不停重连，相当于客户端轮询