# [返回](/)

# 八股文背诵版之—集合容器篇

## List

:point_right:**讲一下List和数组的区别？**

数组是大小固定的一段连续的存储结构，其大小在一开始就已经指定了。它的查询速率非常快，按顺序插入也很快，但是要从中间位置插入或删除，则需要进行数据的拷贝，比较繁琐，并且数组是无法扩容的，而List是支持扩容的，它的底层既有数组实现，也有链表实现；数组可以存放基本数据类型，List不可以

:point_right:**讲一下LinkedList和ArrayList的区别？**

- 从底层存储结构来看：LinkedList采用的是链表的结构；ArrayList采用的是数组的结构
- 从内存占用来看：LinkedList占用内存更多一些，因为包含一些额外的结构比如指向前后的指针（引用）等等
- 从查询效率来看：LinkedList节点内存地址是离散的，作查询的时候，需要从头节点往后遍历直到找到相应的节点；而ArrayList元素内存地址是连续的，可以直接根据索引来查找，时间复杂度低于前者
- 从插入效率来看：LinkedList插入时不需要作数据的复制，只需要插入一个链表节点就可以了；而ArrayList从尾部插入效率也是比较高的，但从中间插入的时候需要移动插入位置后面的所有数据，也就是说需要通过`Arrays.copyOf`复制并移动后面所有的数据，因此代价较高。删除同理
- 从线程安全来看：两者都不是线程安全的

:point_right:**讲一下Vector和ArrayList的区别？**

- 从线程安全来看：ArrayList是非线程安全的，而Vector采用synchronized关键字包装所有方法，因此是线程安全的
- 从扩容机制来看：ArrayList每次扩容为1.5倍；Vector每次扩容为两倍，且能在初始化时通过capacityIncrement指定每次扩容的步长
- 从效率来看：ArrayList略高

:point_right:**你提到了扩容机制，讲讲ArrayList扩容？**

1. 以1.8版本为例，在每次调用add方法添加数据之前，会先调用`ensureCapacityInternal(int minCapacity)`方法，判断是否需要扩容，其中传入方法的参数是size+1，是**最小需要的长度**

   ensureCapacityInternal中会先判断底层的elementData引用是不是指向了一个空数组DEFAULTCAPACITY_EMPTY_ELEMENTDATA，如果是的话，则令`minCapacity=Math.max(minCapacity, DEFAULT_CAPACITY)`，并传到下一个方法`ensureExpicitCapacity`中

2. 在`ensureExplicitCapacity`中，会判断minCapacity是否大于数组长度了，如果是，则调用`grow(int minCapacity)`进行扩容

3. `grow`方法中
   - 首先令newCapacity为旧长度的1.5倍；
   - 如果此时还是小于minCapacity，则直接令newCapacity等于minCapacity；
   - 如果newCapacity大于限定的最大长度，则调用`hugeCapacity(int minCapacity)`方法给newCapacity赋值

4. `hugeCapacity`中

   - 如果minCapacity大于Integer.MAX_VALUE，则抛出OOM
   - 如果大于限定的最大长度MAX_ARRAY_SIZE，则返回Integer.MAX_VALUE
   - 否则返回限定的最大长度

   > ps：MAX_ARRAY_SIZE=Integer.MAX_VALUE - 8，注释上写明由于对象头等原因，预留一些空间出来，减少OOM的可能性

5. 最后通过Arrays.copyOf方法将旧数组的数据拷贝到新数组并返回

   > ps：1.7中无参和有参构造方法会创建一个长度不为0的数组，1.8中直到add的时候，到达Arrays.copy这一步才会创建默认长度或指定长度的数组

:point_right:**为什么ArrayList的elementData用transient修饰？**

意思就是，序列化时，希望在调用默认序列化方法时忽略它。ArrayList重写了writeObject方法，首先调用了defaultWriteObject序列化其他数据，然后再遍历elementData中小于size、即有数据存在的索引处进行序列化。这样做能够减小序列化后占用空间的大小，同时提高序列化效率

:point_right:**知道fast fail机制吗？**

知道，在多个线程同时对一个集合容器进行结构上的操作时，或者在使用迭代器遍历的过程中，进行结构上的操作，可能会抛出ConcurrentModificationException异常，这就是快速失败机制

原理在于，对集合容器进行结构上的操作会令modCount字段++。有些方法在调用之前会令临时变量int expectedModCount=modCount，方法内的逻辑完成之后会调用带参的checkForComodification(expected)方法与modCount进行比对，如果方法逻辑实现过程中，其他线程修改了modCount，此时比对不一致就会抛出上述异常，比如equals、hashCode方法。迭代器在初始化时，会用外部类的modCount赋值给自己的expectedModCount字段，遍历过程中每次调用next方法时，会调用无参的checkForComodification()方法，如果在遍历过程中进行了结构上的修改，就会抛出异常

通过迭代器进行结构上的修改，会同步修改expectedModCount字段，因此是可以的

## Set

:point_right:**讲一下HashSet原理？**

> ps：这类题得整理一套自己的回答逻辑：为什么出现，解决了什么问题？有什么特点、技术点？有什么缺点，用时需要注意什么

Set可以用来无序存储不重复的元素，它不保证元素存储顺序，但能保证元素是非重复的

它的底层是采用HashMap实现的，每次进行add操作时，调用了HashMap的put方法存入一个value固定，存入数据为键的键值对，因此能够通过键和HashMap的特性来保证存入数据的不重复性。固定的value值为final static Object PRESENT = new Object()

需要注意的点有，不重复性是靠hashCode方法和equals来判断的，所以对存入的类型，需要重写hashCode方法和equals方法。如果不重写hashCode方法，只要是不同的对象，都会有它独特的identityhashcode，是内存地址的映射，就算内容相同，也会判断为不同；如果不重写equals方法，会调用默认的Object类的equals对象，同样也是只比较内存地址，不满足实际使用需求

## Map

:point_right:**哈希是什么？哈希冲突是什么？解决哈希冲突的方式？**

将任意长度的输入映射到某一固定长度的输出，这个输出就叫做哈希散列值。哈希冲突是指两个不同的输入产生了相同的哈希值，因此在散列表中存放的位置相同。解决哈希冲突的方式有开放寻址法、拉链法、再哈希法。开放寻址又分为线性探测，平方探测和伪随机探测

:point_right:**HashMap解决哈希冲突的方式？**

① 拉链法解决，相同hash值的数据在一条链表上；② 优化hashcode方法，降低哈希冲突概率；③ 扩容方法分散链表，采用红黑树加快查询等等，有效缓解了哈希冲突下带来的查询效率上的问题

:point_right:**讲一讲HashMap底层结构？**

在1.7版本中，底层是由数组+链表构成的，数组是Entry数组，Entry为链表节点；在1.8中，底层是由数组+链表+红黑树构成的，数组是Node数组，Node为链表节点，它的子类TreeNode为红黑树节点

:point_right:**你提到了1.7、1.8版本，区别？1.7有哪些问题？1.8呢？**

**区别**

一个个环节来捋吧。首先是底层结构，一个是Entry数组，一个是Node数组

其次是初始化方法时，1.7中会在put时调用inflateTable(threshold)来创建数组，而1.8中put时，数组创建直接集成到了resize()方法；在初始化时，会根据capacity参数确认实际的容量，为2的n次方，便于快速确定索引，1.7中是使用numberOfLeadingZeros依此移位并且比较，1.8中调用的tableSizeFor方法确定，通过固定几次移位就能达到相同效果，不用比较，效率更高

接着是put方法，1.7的hash方法扰动了8次，4次位运算4次异或运算，1.8只扰动了2次，效率更高；1.8中的链表深度>8并且总节点数>64时，该链表会进行树化，提高查找效率，而1.7不会；1.7中会先判断需不需要扩容，先扩容再createEntry添加，1.8中则是先完成Node的添加之后再判断需不需要扩容

接着是扩容方法，1.7的扩容时的数据迁移是遍历数组，对于有数据的链表，遍历链表并计算一个个节点的新位置，再一个个用头插法插入到新位置，链表顺序会倒转，1.8中迁移时，当遍历到有数据的数组元素，则维护两个链表，头节点分别是loHead和hiHead，代表在新数组中位置不变还是+oldCap，通过位运算分别将该链表的节点挂到loHead或hiHead上，最后一次性令新数组的原位置指向loHead，原位置+旧数组长度的位置指向hiHead，不会改变链表顺序；1.8的扩容时可能会调用split方法进行反树化

**1.7问题**

由于头插法的弊端，扩容数据迁移时可能产生循环链表和数据丢失的问题，还会由于并发性问题产生数据覆盖

**1.8问题**

1.8中改成尾插法，解决了循环链表和数据丢失问题，但还可能会发生数据覆盖的问题，比如多线程++size操作只加了一次

:point_right:**讲一下put方法流程？**

以1.8版本为例。首先会判断table是不是为空，如果是，则调用resize()方法创建一个新table；接着会判断对应位置是不是指向空，如果是，则直接创建一个新Node并指向它；否则进入下一个代码块

下一个代码块中，首先判断该位置头节点和要插入数据的hash、equals，相等的话则直接令临时变量e等于该头节点；否则会判断头节点是否为TreeNode，如果是，则调用TreeNode对应的put方法，并将返回结果赋给e；再否则则进入遍历循环，每次令e指向节点的next，同时累加bitCount，如果中途某个节点hash、equals与插入节点相等，则令e等于该节点，并break，没有的话则遍历到e等于null，创建一个新Node并令当前节点的next指向它，如果此时链表深度大于8，则进行树化

最后，判断e是不是null，不为null表示找到了hash、equals都一致的节点，就用新值替换旧值并返回；如果为null，表示插入了新的节点，则判断++size有没有大于threshold，如果大于，则要进行扩容和数据迁移

1.7逻辑比较简单，先put判断table是否为空，为空则inflateTable(threshold)，再判断有没有重复节点，有就替换并返回；没有就addEntry，addEntry先判断要不要扩容，再createEntry添加节点，在createEntry中令size++

:point_right:**手写？**

> 建议手写1.7版本的，put逻辑和resize逻辑简单一些，可以加入1.8中一些优点

> 这里给出两种版本的

1.7版本，只实现了put方法，含扩容等等

```java
import java.util.AbstractMap;
import java.util.Map;
import java.util.Set;

/**
 * @ClassName: MyMap17
 * @Description:
 * @Author: 74759
 * @Date: 2021/12/29 21:40
 */
public class MyMap17<K, V> extends AbstractMap<K, V> implements Map<K, V> {
    static class Entry17<K, V> implements Map.Entry<K, V>{
        K key;
        V value;
        int hash;
        Entry17<K, V> next;

        public Entry17(K key, V value, int hash, Entry17<K, V> next) {
            this.key = key;
            this.value = value;
            this.hash = hash;
            this.next = next;
        }

        @Override
        public K getKey() {
            return key;
        }

        @Override
        public V getValue() {
            return value;
        }

        @Override
        public V setValue(V value) {
            V oldValue = value;
            this.value = value;
            return oldValue;
        }
    }
    int size;
    int threshold;
    float loadFactor;
    transient private Entry17<K, V>[] table;

    public MyMap17(int capacity, float loadFactor) {
        this.loadFactor = loadFactor;
        int c = 1;
        while (c < capacity){
            c <<= 1;
        }
        table = new Entry17[c];
        this.threshold = (int) (c * loadFactor);
    }
    public MyMap17(int capacity) {
        this(capacity, 0.75f);
    }
    public MyMap17() {
        this(16);
    }
    @Override
    public V put(K key, V value){
        int hash = hash(key);
        int index = hash & (table.length - 1);
        for (Entry17<K, V> e = table[index]; e != null; e = e.next){
            if (e.hash == hash && (e.key == key || key.equals(e.key))){
                V oldValue = e.value;
                e.value = value;
                return oldValue;
            }
        }
        addEntry(key, value, hash, index);
        return null;
    }

    private void addEntry(K key, V value, int hash, int bucketIndex) {
        if (size >= threshold && table[bucketIndex] != null){
            resize(table.length << 1);
        }
        bucketIndex = hash & (table.length - 1);
        createEntry(key, value, hash, bucketIndex);
    }

    private void createEntry(K key, V value, int hash, int bucketIndex) {
        table[bucketIndex] = new Entry17<>(key, value, hash, table[bucketIndex]);
        ++size;
    }

    private void resize(int newCap) {
        Entry17<K, V>[] newTab = new Entry17[newCap];
        transfer(false, newTab);
        threshold = (int) (newCap * loadFactor);
        table = newTab;
    }

    private void transfer(boolean rehash, Entry17<K,V>[] newTab) {
        for (Entry17<K, V> e: table){
            while (e != null){
                Entry17<K, V> next = e.next;
                if (rehash){
                    e.hash = hash(e.key);
                }
                int newBucketIndex = e.hash & (newTab.length - 1);
                e.next = newTab[newBucketIndex];
                newTab[newBucketIndex] = e;
                e = next;
            }
        }
    }
    public int hash(Object k){
        int h = k.hashCode();
        h ^= (h >>> 20) ^ (h >>> 12);
        return h ^ (h >>> 7) ^ (h >>> 4);
    }
    @Override
    public Set<Entry<K, V>> entrySet() {
        return null;
    }
}
```

1.8版本，同样是只实现了put方法

```java
import java.util.AbstractMap;
import java.util.Map;
import java.util.Set;

/**
 * @ClassName: MyMap18
 * @Description:
 * @Author: 74759
 * @Date: 2021/12/29 22:11
 */
public class MyMap18<K, V> extends AbstractMap<K, V> implements Map<K, V> {
    static class Node18<K, V> implements Map.Entry<K, V>{
        K key;
        V value;
        int hash;
        Node18<K, V> next;
        @Override
        public K getKey() {
            return key;
        }

        @Override
        public V getValue() {
            return value;
        }

        @Override
        public V setValue(V value) {
            V oldValue = this.value;
            this.value = value;
            return oldValue;
        }

        public Node18(K key, V value, int hash, Node18<K, V> next) {
            this.key = key;
            this.value = value;
            this.hash = hash;
            this.next = next;
        }

    }
    int size;
    int threshold;
    float loadFactor;
    transient Node18<K, V>[] table;

    public MyMap18(int capacity, float loadFactor) {
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(capacity);
    }
    public MyMap18(int capacity) {
        this(capacity, 0.75f);
    }
    public MyMap18() {
        this(16);
    }
    @Override
    public V put(K key, V value){
        return putVal(hash(key), key, value, false, true);
    }

    private int hash(Object key) {
        int h;
        return null == key ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

    private V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {
        int n, i;
        Node18<K, V> p;
        if (table == null || (n = table.length) == 0){
            n = (table = resize()).length;
        }
        if (table[(i = hash & (n - 1))] == null){
            table[i] = new Node18<>(key, value, hash, null);
        }else {
            Node18<K, V> e;
            if ((p = table[i]).hash == hash && (p.key == key || (key != null && key.equals(p.key)))){
                e = p;
            }else if (p instanceof MyTreeNode){
                e = ((MyTreeNode<K, V>) p).putTreeNode(hash, key, value, false, true);
            }else {
                for (int bitCount = 0; ; ++bitCount){
                    if ((e = p.next) == null){
                        p.next = new Node18<>(key, value, hash, null);
                        if (bitCount >= 7){
                            treeifyBin(hash, table);
                        }
                    }
                    if (e.hash == hash && (e.key == key || (key != null && key.equals(e.key)))){
                        break;
                    }
                    p = e;
                }
            }
            if (e != null){
                V oldValue = e.value;
                if (onlyIfAbsent){
                    e.value = value;
                }
                return oldValue;
            }
        }
        if (++size > threshold){
            resize();
        }
        return null;
    }

    private void treeifyBin(int hash, Node18<K,V>[] table) {
    }

    private Node18<K, V>[] resize() {
        int oldCap = table == null ? 0 : table.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0){
            if (oldCap >= (1 << 30)){
                threshold = Integer.MAX_VALUE;
                return table;
            }
            if ((newCap = oldCap << 1) <= (1 << 30) && oldCap >= 16){
                newThr = oldThr << 1;
            }
        }else {
            if (oldThr > 0){
                newCap = oldThr;
            }else {
                newCap = 16;
                newThr = (int) (newCap * 0.75f);
            }
        }
        if (newThr == 0){
            float ft = newCap * 0.75f;
            newThr = newCap <= (1 << 30) && ft <= (1 << 30) ? (int) ft : Integer.MAX_VALUE;
        }
        threshold = newThr;
        Node18<K, V>[] newTab = new Node18[newCap];
        if (table != null){
            for (int i = 0; i < oldCap; i++){
                Node18<K, V> e;
                if ((e = table[i]) != null){
                    if (e.next == null){
                        newTab[e.hash & (newCap - 1)] = e;
                    }else if (e instanceof MyTreeNode){
                        ((MyTreeNode<K, V>) e).split(this, newTab, i, oldCap);
                    }else {
                        Node18<K, V> next;
                        Node18<K, V> lh = null, lt = null, hh = null, ht = null;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0){
                                if (lt == null){
                                    lh = e;
                                }else {
                                    lt.next = e;
                                }
                                lt = e;
                            }else {
                                if (ht == null){
                                    hh = e;
                                }else {
                                    ht.next = e;
                                }
                                ht = e;
                            }
                            e = next;
                        }while (next != null);
                        if (lt != null){
                            lt.next = null;
                            newTab[i] = lh;
                        }
                        if (ht != null){
                            ht.next = null;
                            newTab[i + oldCap] = hh;
                        }
                    }
                }
            }
        }
        table = newTab;
        return newTab;
    }

    private int tableSizeFor(int capacity) {
        int n = capacity - 1;
        n |= (n >> 1);
        n |= (n >> 2);
        n |= (n >> 4);
        n |= (n >> 8);
        n |= (n >> 16);
        return n < 0 ? 1 : n + 1;
    }

    @Override
    public Set<Entry<K, V>> entrySet() {
        return null;
    }
}
class MyTreeNode<K, V> extends MyMap18.Node18<K, V> {

    public MyTreeNode(K key, V value, int hash, MyMap18.Node18<K, V> next) {
        super(key, value, hash, next);
    }
    public MyTreeNode<K,V> putTreeNode(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {
        return null;
    }
    public void split(MyMap18<K, V> map, MyMap18.Node18<K, V>[] newTab, int oldBucketIndex, int oldCap){}
}
```

:point_right:**讲一讲HashMap原理？**

HashMap是Map接口下一个常用的类，用于存放以键值对形式存储的双列数据

它能够保证Key的不重复性，且能够以很快的效率实现查询

它的底层是通过数组+链表+红黑树来存储数据的，数组是Node数组，每个元素都是一个链表节点，在链表长度大于8且总元素数大于64时会将该链表转换成红黑树（顺便讲讲put）。它是用拉链法来解决哈希冲突的（讲讲hash方法）。当存储数据大于threshold，会进行扩容（讲讲扩容）

HashMap1.7中容易出现循环链表、数据丢失、数据覆盖等问题；1.8中会出现数据覆盖的问题

使用时注意Key的hashcode方法和equals方法的重写

:point_right:**讲一讲LinkedHashMap？**

LinkedHashMap是HashMap的一个子类，在继承了HashMap的基本方法以外，还提供了保存插入或访问map数据的顺序的功能以及删除最老节点等功能

LinkedHashMap底层是采用Entry数组实现的，这里面的Entry继承了HashMap中的Node类，在Node类的基础上，增加了before和after引用，用于基于插入的顺序维护一个双向链表。LinkedHashMap重写了HashMap的newNode方法，每次put的时候，将元素插入到双向链表的尾部。此外还提供了按访问顺序排序的功能，在初始化时指定accessOrder属性为true，调用put等方法在map中能够找到对应节点时，会调用afterNodeAccess方法，将访问到的节点放到双向链表的尾部。此外还提供了删除最老结点的功能，比如调用HashMap的put之后，插入新节点后会调用afterNodeInsertion(evict)方法，通过`evict&&head节点不等于null&&removeEldestEntry(first)`来判断要不要调用removeNode方法来删除first节点；removeEldestEntry方法在LinkedHashMap中是空方法，可以子类中重写该方法来定义删除最老节点的规则，可以通过accessOrder和该方法的配合简单实现LRU

:point_right:**手写LRU？**

简单通过LinkedHashMap实现

```java
import java.util.*;

public class Testt {
    public static void main(String[] args) throws InterruptedException {
        SimpleLRU<String, String> lru = new SimpleLRU<>();
        lru.put("s1", "hello");
        lru.put("s2", "world");
        lru.put("s3", "!!!");
        lru.put("s4", "say");
        lru.put("s5", "hi");
        lru.get("s1");
        // 此时排列为s2->s3->s4->s5->s1
        lru.put("s6", "hah");
        for (Map.Entry<String, String> entry: lru.entrySet()){
            System.out.println(entry.getKey() + "->" + entry.getValue());
        }
    }
}
class SimpleLRU<K, V> extends LinkedHashMap<K, V>{
    public SimpleLRU(int initialCapacity) {
        super(initialCapacity, 0.75f, true);
    }
    public SimpleLRU() {
        super(16, 0.75f, true);
    }
    public SimpleLRU(int initialCapacity, float loadFactor) {
        super(initialCapacity, loadFactor, true);
    }

    /**
     * size > 5，删除最老访问节点
     * @param eldest
     * @return
     */
    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        return size() > 5;
    }
}
```

![image-20220104174141705](imgs\1.png)

> 一般面试不会让这么写了，但可以借鉴思想

升级版手写LRU

```java
class LRUCache {
    Entry head, tail;
    Map<Integer, Entry> map;
    int limit;
    int size;
    public LRUCache(int capacity) {
        this.limit = capacity;
        map = new HashMap<>(capacity << 1);
    }
    public void moveToTail(Entry p){
        Entry b = p.before, a = p.after, last = tail;
        p.after = null;
        if (b == null)
            head = a;
        else
            b.after = a;
        if (a != null)
            a.before = b;
        else
            last = b;
        if (last == null)
            head = p;
        else {
            p.before = last;
            last.after = p;
        }
        tail = p;
    }
    public int get(int key) {
        Entry p = map.getOrDefault(key, null);
        if(null == p){
            return -1;
        }
        moveToTail(p);
        return p.val;
    }
    
    public void put(int key, int value) {
        Entry p;
        if(map.containsKey(key)){
            p = map.get(key);
            moveToTail(p);
            p.val = value;
        }else{
            if(tail == null){
                head = tail = p = new Entry(key, value, null, null);
            }else{
                tail.after = p = new Entry(key, value, tail, null);
                tail = tail.after;
            }
            map.put(key, p);
            if(++size > limit){
                int k = head.key;
                head = head.after;
                head.before = null;
                map.remove(k);
            }
        }
    }
    static class Entry{
        int key;
        int val;
        Entry before;
        Entry after;
        Entry(int key, int val, Entry before, Entry after){
            this.key = key;
            this.val = val;
            this.before = before;
            this.after = after;
        }
    }
}
```

:point_right:**​HashMap为什么用红黑树不用AVL树？**

> 简单介绍AVL树、红黑树→比较区别→为什么用红黑树

AVL树是带有平衡条件的二叉查找树，一般是用平衡因子差值判断是否平衡并通过旋转来实现平衡，左右子树树高不超过1，**和红黑树相比，AVL树是严格的平衡二叉树**，平衡条件必须满足。不管我们是执行插入还是删除操作，只要不满足上面的条件，就要通过旋转来保持平衡，而**旋转是非常耗时**的，由此我们可以知道AVL树适合用于插入与删除次数比较少，但查找多的情况

红黑树是一种二叉查找树，但在每个节点增加一个存储位表示节点的颜色，可以是红或黑。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，红黑树确保没有一条路径会比其它路径长出两倍，因此，红黑树是一种弱平衡二叉树，相对于要求严格的AVL树来说，它的旋转次数少，**所以对于搜索，插入，删除操作较多的情况下，就用红黑树**

由于维护AVL的高度平衡所付出的代价比从中获得的效率收益还大，故而实际的应用不多，更多的地方是用追求局部而不是非常严格整体平衡的红黑树

